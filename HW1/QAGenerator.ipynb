{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Couldn't instantiate the backend tokenizer from one of: \n(1) a `tokenizers` library serialization file, \n(2) a slow tokenizer instance to convert or \n(3) an equivalent slow tokenizer class to instantiate and convert. \nYou need to have sentencepiece installed to convert a slow tokenizer to a fast one.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/jwa/Desktop/Code/CMUCode/NLP/HW1/QAGenerator.ipynb 셀 1\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jwa/Desktop/Code/CMUCode/NLP/HW1/QAGenerator.ipynb#W0sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m AutoModelWithLMHead, AutoTokenizer\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jwa/Desktop/Code/CMUCode/NLP/HW1/QAGenerator.ipynb#W0sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m \n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/jwa/Desktop/Code/CMUCode/NLP/HW1/QAGenerator.ipynb#W0sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m tokenizer \u001b[39m=\u001b[39m AutoTokenizer\u001b[39m.\u001b[39;49mfrom_pretrained(\u001b[39m\"\u001b[39;49m\u001b[39mmrm8488/t5-base-finetuned-question-generation-ap\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jwa/Desktop/Code/CMUCode/NLP/HW1/QAGenerator.ipynb#W0sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m model \u001b[39m=\u001b[39m AutoModelWithLMHead\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m\"\u001b[39m\u001b[39mmrm8488/t5-base-finetuned-question-generation-ap\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/NLP/lib/python3.9/site-packages/transformers/models/auto/tokenization_auto.py:628\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    626\u001b[0m tokenizer_class_py, tokenizer_class_fast \u001b[39m=\u001b[39m TOKENIZER_MAPPING[\u001b[39mtype\u001b[39m(config)]\n\u001b[1;32m    627\u001b[0m \u001b[39mif\u001b[39;00m tokenizer_class_fast \u001b[39mand\u001b[39;00m (use_fast \u001b[39mor\u001b[39;00m tokenizer_class_py \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m):\n\u001b[0;32m--> 628\u001b[0m     \u001b[39mreturn\u001b[39;00m tokenizer_class_fast\u001b[39m.\u001b[39;49mfrom_pretrained(pretrained_model_name_or_path, \u001b[39m*\u001b[39;49minputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    629\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    630\u001b[0m     \u001b[39mif\u001b[39;00m tokenizer_class_py \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/NLP/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1775\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1772\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1773\u001b[0m         logger\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mloading file \u001b[39m\u001b[39m{\u001b[39;00mfile_path\u001b[39m}\u001b[39;00m\u001b[39m from cache at \u001b[39m\u001b[39m{\u001b[39;00mresolved_vocab_files[file_id]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 1775\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m_from_pretrained(\n\u001b[1;32m   1776\u001b[0m     resolved_vocab_files,\n\u001b[1;32m   1777\u001b[0m     pretrained_model_name_or_path,\n\u001b[1;32m   1778\u001b[0m     init_configuration,\n\u001b[1;32m   1779\u001b[0m     \u001b[39m*\u001b[39;49minit_inputs,\n\u001b[1;32m   1780\u001b[0m     use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[1;32m   1781\u001b[0m     cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m   1782\u001b[0m     local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[1;32m   1783\u001b[0m     _commit_hash\u001b[39m=\u001b[39;49mcommit_hash,\n\u001b[1;32m   1784\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m   1785\u001b[0m )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/NLP/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1930\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._from_pretrained\u001b[0;34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, use_auth_token, cache_dir, local_files_only, _commit_hash, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1928\u001b[0m \u001b[39m# Instantiate tokenizer.\u001b[39;00m\n\u001b[1;32m   1929\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1930\u001b[0m     tokenizer \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m(\u001b[39m*\u001b[39;49minit_inputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minit_kwargs)\n\u001b[1;32m   1931\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mOSError\u001b[39;00m:\n\u001b[1;32m   1932\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mOSError\u001b[39;00m(\n\u001b[1;32m   1933\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mUnable to load vocabulary from file. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1934\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mPlease check that the provided vocabulary is accessible and not corrupted.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1935\u001b[0m     )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/NLP/lib/python3.9/site-packages/transformers/models/t5/tokenization_t5_fast.py:134\u001b[0m, in \u001b[0;36mT5TokenizerFast.__init__\u001b[0;34m(self, vocab_file, tokenizer_file, eos_token, unk_token, pad_token, extra_ids, additional_special_tokens, **kwargs)\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[39mif\u001b[39;00m extra_tokens \u001b[39m!=\u001b[39m extra_ids:\n\u001b[1;32m    128\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    129\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mBoth extra_ids (\u001b[39m\u001b[39m{\u001b[39;00mextra_ids\u001b[39m}\u001b[39;00m\u001b[39m) and additional_special_tokens (\u001b[39m\u001b[39m{\u001b[39;00madditional_special_tokens\u001b[39m}\u001b[39;00m\u001b[39m) are\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    130\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m provided to T5Tokenizer. In this case the additional_special_tokens must include the extra_ids\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    131\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m tokens\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    132\u001b[0m         )\n\u001b[0;32m--> 134\u001b[0m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\n\u001b[1;32m    135\u001b[0m     vocab_file,\n\u001b[1;32m    136\u001b[0m     tokenizer_file\u001b[39m=\u001b[39;49mtokenizer_file,\n\u001b[1;32m    137\u001b[0m     eos_token\u001b[39m=\u001b[39;49meos_token,\n\u001b[1;32m    138\u001b[0m     unk_token\u001b[39m=\u001b[39;49munk_token,\n\u001b[1;32m    139\u001b[0m     pad_token\u001b[39m=\u001b[39;49mpad_token,\n\u001b[1;32m    140\u001b[0m     extra_ids\u001b[39m=\u001b[39;49mextra_ids,\n\u001b[1;32m    141\u001b[0m     additional_special_tokens\u001b[39m=\u001b[39;49madditional_special_tokens,\n\u001b[1;32m    142\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    143\u001b[0m )\n\u001b[1;32m    145\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvocab_file \u001b[39m=\u001b[39m vocab_file\n\u001b[1;32m    146\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcan_save_slow_tokenizer \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvocab_file \u001b[39melse\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/NLP/lib/python3.9/site-packages/transformers/tokenization_utils_fast.py:120\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m     fast_tokenizer \u001b[39m=\u001b[39m convert_slow_tokenizer(slow_tokenizer)\n\u001b[1;32m    119\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 120\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    121\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mCouldn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt instantiate the backend tokenizer from one of: \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    122\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m(1) a `tokenizers` library serialization file, \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    123\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m(2) a slow tokenizer instance to convert or \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    124\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m(3) an equivalent slow tokenizer class to instantiate and convert. \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    125\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mYou need to have sentencepiece installed to convert a slow tokenizer to a fast one.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    126\u001b[0m     )\n\u001b[1;32m    128\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tokenizer \u001b[39m=\u001b[39m fast_tokenizer\n\u001b[1;32m    130\u001b[0m \u001b[39mif\u001b[39;00m slow_tokenizer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mValueError\u001b[0m: Couldn't instantiate the backend tokenizer from one of: \n(1) a `tokenizers` library serialization file, \n(2) a slow tokenizer instance to convert or \n(3) an equivalent slow tokenizer class to instantiate and convert. \nYou need to have sentencepiece installed to convert a slow tokenizer to a fast one."
     ]
    }
   ],
   "source": [
    "# Tip: By now, install transformers from source\n",
    "from transformers import AutoModelWithLMHead, AutoTokenizer\n",
    "import pandas as pd \n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mrm8488/t5-base-finetuned-question-generation-ap\")\n",
    "model = AutoModelWithLMHead.from_pretrained(\"mrm8488/t5-base-finetuned-question-generation-ap\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_question(answer, context, max_length=64):\n",
    "    input_text = \"answer: %s  context: %s </s>\" % (answer, context)\n",
    "    features = tokenizer([input_text], return_tensors='pt')\n",
    "\n",
    "    output = model.generate(input_ids=features['input_ids'], \n",
    "                  attention_mask=features['attention_mask'],\n",
    "                  max_length=max_length)\n",
    "\n",
    "    return tokenizer.decode(output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qa_generation(context):\n",
    "    print(\"context: \" + context)\n",
    "    answer = input(\"answer: \")\n",
    "    print(answer)\n",
    "    question = get_question(answer, context)[16:-4]\n",
    "    print(\"question: \" + question)\n",
    "    add_question = input(\"add some word in questions : \")\n",
    "    difficulty = input(\"difficulty(e,m,h,f): \")\n",
    "    print(difficulty + '\\n')\n",
    "\n",
    "    return answer, question + '' + add_question, difficulty.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_extract(dir):\n",
    "    document = [] \n",
    "    with open(\"documents\"+dir, \"r\") as file:\n",
    "        while True:\n",
    "            line = file.readline()\n",
    "            if not line:\n",
    "                break\n",
    "            if line != '\\n' and line[0] != '=':\n",
    "                sentence = line.strip('\\n').split(\". \")\n",
    "                document.extend(sentence)\n",
    "    \n",
    "    return document\n",
    "\n",
    "def sentence_extract2(dir):\n",
    "    document = [] \n",
    "    with open(\"documents\"+dir, \"r\") as file:\n",
    "        while True:\n",
    "            line = file.readline()\n",
    "            if not line:\n",
    "                break\n",
    "            if line != '\\n' and line[0] != '=':\n",
    "                sentence = line.strip('\\n').split(\". \")\n",
    "                document.extend(sentence)\n",
    "\n",
    "    two_sentence = ''\n",
    "    new_document = []\n",
    "    for i in document:\n",
    "        if two_sentence == '':\n",
    "            two_sentence = i\n",
    "        else:\n",
    "            new_document.append(two_sentence + '. ' + i + '.')\n",
    "            two_sentence = ''\n",
    "    \n",
    "    return new_document\n",
    "\n",
    "def sentence_extract3(dir):\n",
    "    document = [] \n",
    "    with open(\"documents\"+dir, \"r\") as file:\n",
    "        while True:\n",
    "            line = file.readline()\n",
    "            if not line:\n",
    "                break\n",
    "            if line != '\\n' and line[0] != '=':\n",
    "                sentence = line.strip('\\n').split(\". \")\n",
    "                document.extend(sentence)\n",
    "\n",
    "    num_sentence = 0\n",
    "    sentence = ''\n",
    "    new_document = []\n",
    "    for i in document:\n",
    "        if num_sentence == 0:\n",
    "            sentence = i\n",
    "            num_sentence += 1\n",
    "        elif num_sentence < 2:\n",
    "            sentence = sentence + '. ' + i\n",
    "            num_sentence += 1\n",
    "        else:\n",
    "            new_document.append(sentence + '. ' + i)\n",
    "            sentence = ''\n",
    "            num_sentence = 0\n",
    "    \n",
    "    return new_document\n",
    "dir = \"/pokemon_characters/eevee_pokemon.txt\"\n",
    "document = sentence_extract3(dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context: Author Loredana Lipperini noted Eevee as being one of the \"most mysterious Pokémon in the series\". Technology Tell writer Jenni Lada noted that Eevee is a character \"whom should feature\" in Super Smash Bros., citing its customization \"potential\". Liz Finnegan of The Escapist listed Eevee as their eleventh favorite Pokémon, stating that Eevee is like the evolutionary equivalent of the turducken\n",
      "yes\n",
      "question: Is Eevee a good character for the Super Smash Bros.?\n",
      "e\n",
      "\n",
      "context: Gita Jackson of Kotaku claimed that Eevee is what Pokémon is all about, stating that \"Eevee is adorable, but that’s not the only reason why I love them. Somewhere between a cat and a dog, Eevee seems more domesticated than longtime series mascot Pikachu, but still as mischievous as any other wild animal.\"In 2015, Eevee was the most traded Pokémon in the games' \"Wonder Trade\" feature. A special Pokémon Omega Ruby and Alpha Sapphire Online Competition known as the \"Eevee Friendly Match\" limits participants' Pokémon to Eevee and its evolutions only\n",
      "yes\n",
      "question: Is Eevee a Pokémon?\n",
      "e\n",
      "\n"
     ]
    }
   ],
   "source": [
    "answers = []\n",
    "questions = []\n",
    "documents = []\n",
    "difficulties = []\n",
    "for i, sentence in enumerate(document):\n",
    "    if i < 20:\n",
    "        continue\n",
    "    elif i == 22:\n",
    "        break\n",
    "    a, q, d = qa_generation(sentence)\n",
    "\n",
    "    if d =='F':\n",
    "        continue\n",
    "        \n",
    "    answers.append(a)\n",
    "    questions.append(q)\n",
    "    difficulties.append(d) # E, M, H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict = {'document_id': dir, 'question': questions, 'answer': answers, 'difficulty': difficulties}  \n",
    "df = pd.DataFrame(dict) \n",
    "df.to_csv('test.csv', index=False)  \n",
    "#df.to_csv('test.tsv', index=False, sep=\"\\t\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv ('result_4.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "M    40\n",
       "E    30\n",
       "H    30\n",
       "Name: difficulty, dtype: int64"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1['difficulty'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('NLP')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c6584a8ba916643306bb5090a7c6d906392e220f89e7f1e5ab1565c1ede3c150"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
