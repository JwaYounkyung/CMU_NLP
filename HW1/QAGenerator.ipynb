{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Couldn't instantiate the backend tokenizer from one of: \n(1) a `tokenizers` library serialization file, \n(2) a slow tokenizer instance to convert or \n(3) an equivalent slow tokenizer class to instantiate and convert. \nYou need to have sentencepiece installed to convert a slow tokenizer to a fast one.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/jwa/Desktop/Code/CMUCode/NLP/HW1/QAGenerator.ipynb ì…€ 1\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jwa/Desktop/Code/CMUCode/NLP/HW1/QAGenerator.ipynb#W0sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m AutoModelWithLMHead, AutoTokenizer\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jwa/Desktop/Code/CMUCode/NLP/HW1/QAGenerator.ipynb#W0sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m \n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/jwa/Desktop/Code/CMUCode/NLP/HW1/QAGenerator.ipynb#W0sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m tokenizer \u001b[39m=\u001b[39m AutoTokenizer\u001b[39m.\u001b[39;49mfrom_pretrained(\u001b[39m\"\u001b[39;49m\u001b[39mmrm8488/t5-base-finetuned-question-generation-ap\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jwa/Desktop/Code/CMUCode/NLP/HW1/QAGenerator.ipynb#W0sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m model \u001b[39m=\u001b[39m AutoModelWithLMHead\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m\"\u001b[39m\u001b[39mmrm8488/t5-base-finetuned-question-generation-ap\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/NLP/lib/python3.9/site-packages/transformers/models/auto/tokenization_auto.py:628\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    626\u001b[0m tokenizer_class_py, tokenizer_class_fast \u001b[39m=\u001b[39m TOKENIZER_MAPPING[\u001b[39mtype\u001b[39m(config)]\n\u001b[1;32m    627\u001b[0m \u001b[39mif\u001b[39;00m tokenizer_class_fast \u001b[39mand\u001b[39;00m (use_fast \u001b[39mor\u001b[39;00m tokenizer_class_py \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m):\n\u001b[0;32m--> 628\u001b[0m     \u001b[39mreturn\u001b[39;00m tokenizer_class_fast\u001b[39m.\u001b[39;49mfrom_pretrained(pretrained_model_name_or_path, \u001b[39m*\u001b[39;49minputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    629\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    630\u001b[0m     \u001b[39mif\u001b[39;00m tokenizer_class_py \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/NLP/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1775\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1772\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1773\u001b[0m         logger\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mloading file \u001b[39m\u001b[39m{\u001b[39;00mfile_path\u001b[39m}\u001b[39;00m\u001b[39m from cache at \u001b[39m\u001b[39m{\u001b[39;00mresolved_vocab_files[file_id]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 1775\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m_from_pretrained(\n\u001b[1;32m   1776\u001b[0m     resolved_vocab_files,\n\u001b[1;32m   1777\u001b[0m     pretrained_model_name_or_path,\n\u001b[1;32m   1778\u001b[0m     init_configuration,\n\u001b[1;32m   1779\u001b[0m     \u001b[39m*\u001b[39;49minit_inputs,\n\u001b[1;32m   1780\u001b[0m     use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[1;32m   1781\u001b[0m     cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m   1782\u001b[0m     local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[1;32m   1783\u001b[0m     _commit_hash\u001b[39m=\u001b[39;49mcommit_hash,\n\u001b[1;32m   1784\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m   1785\u001b[0m )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/NLP/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1930\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._from_pretrained\u001b[0;34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, use_auth_token, cache_dir, local_files_only, _commit_hash, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1928\u001b[0m \u001b[39m# Instantiate tokenizer.\u001b[39;00m\n\u001b[1;32m   1929\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1930\u001b[0m     tokenizer \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m(\u001b[39m*\u001b[39;49minit_inputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minit_kwargs)\n\u001b[1;32m   1931\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mOSError\u001b[39;00m:\n\u001b[1;32m   1932\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mOSError\u001b[39;00m(\n\u001b[1;32m   1933\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mUnable to load vocabulary from file. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1934\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mPlease check that the provided vocabulary is accessible and not corrupted.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1935\u001b[0m     )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/NLP/lib/python3.9/site-packages/transformers/models/t5/tokenization_t5_fast.py:134\u001b[0m, in \u001b[0;36mT5TokenizerFast.__init__\u001b[0;34m(self, vocab_file, tokenizer_file, eos_token, unk_token, pad_token, extra_ids, additional_special_tokens, **kwargs)\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[39mif\u001b[39;00m extra_tokens \u001b[39m!=\u001b[39m extra_ids:\n\u001b[1;32m    128\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    129\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mBoth extra_ids (\u001b[39m\u001b[39m{\u001b[39;00mextra_ids\u001b[39m}\u001b[39;00m\u001b[39m) and additional_special_tokens (\u001b[39m\u001b[39m{\u001b[39;00madditional_special_tokens\u001b[39m}\u001b[39;00m\u001b[39m) are\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    130\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m provided to T5Tokenizer. In this case the additional_special_tokens must include the extra_ids\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    131\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m tokens\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    132\u001b[0m         )\n\u001b[0;32m--> 134\u001b[0m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\n\u001b[1;32m    135\u001b[0m     vocab_file,\n\u001b[1;32m    136\u001b[0m     tokenizer_file\u001b[39m=\u001b[39;49mtokenizer_file,\n\u001b[1;32m    137\u001b[0m     eos_token\u001b[39m=\u001b[39;49meos_token,\n\u001b[1;32m    138\u001b[0m     unk_token\u001b[39m=\u001b[39;49munk_token,\n\u001b[1;32m    139\u001b[0m     pad_token\u001b[39m=\u001b[39;49mpad_token,\n\u001b[1;32m    140\u001b[0m     extra_ids\u001b[39m=\u001b[39;49mextra_ids,\n\u001b[1;32m    141\u001b[0m     additional_special_tokens\u001b[39m=\u001b[39;49madditional_special_tokens,\n\u001b[1;32m    142\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    143\u001b[0m )\n\u001b[1;32m    145\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvocab_file \u001b[39m=\u001b[39m vocab_file\n\u001b[1;32m    146\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcan_save_slow_tokenizer \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvocab_file \u001b[39melse\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/NLP/lib/python3.9/site-packages/transformers/tokenization_utils_fast.py:120\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m     fast_tokenizer \u001b[39m=\u001b[39m convert_slow_tokenizer(slow_tokenizer)\n\u001b[1;32m    119\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 120\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    121\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mCouldn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt instantiate the backend tokenizer from one of: \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    122\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m(1) a `tokenizers` library serialization file, \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    123\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m(2) a slow tokenizer instance to convert or \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    124\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m(3) an equivalent slow tokenizer class to instantiate and convert. \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    125\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mYou need to have sentencepiece installed to convert a slow tokenizer to a fast one.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    126\u001b[0m     )\n\u001b[1;32m    128\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tokenizer \u001b[39m=\u001b[39m fast_tokenizer\n\u001b[1;32m    130\u001b[0m \u001b[39mif\u001b[39;00m slow_tokenizer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mValueError\u001b[0m: Couldn't instantiate the backend tokenizer from one of: \n(1) a `tokenizers` library serialization file, \n(2) a slow tokenizer instance to convert or \n(3) an equivalent slow tokenizer class to instantiate and convert. \nYou need to have sentencepiece installed to convert a slow tokenizer to a fast one."
     ]
    }
   ],
   "source": [
    "# Tip: By now, install transformers from source\n",
    "from transformers import AutoModelWithLMHead, AutoTokenizer\n",
    "import pandas as pd \n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mrm8488/t5-base-finetuned-question-generation-ap\")\n",
    "model = AutoModelWithLMHead.from_pretrained(\"mrm8488/t5-base-finetuned-question-generation-ap\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_question(answer, context, max_length=64):\n",
    "    input_text = \"answer: %s  context: %s </s>\" % (answer, context)\n",
    "    features = tokenizer([input_text], return_tensors='pt')\n",
    "\n",
    "    output = model.generate(input_ids=features['input_ids'], \n",
    "                  attention_mask=features['attention_mask'],\n",
    "                  max_length=max_length)\n",
    "\n",
    "    return tokenizer.decode(output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qa_generation(context):\n",
    "    print(\"context: \" + context)\n",
    "    answer = input(\"answer: \")\n",
    "    print(answer)\n",
    "    question = get_question(answer, context)[16:-4]\n",
    "    print(\"question: \" + question)\n",
    "    add_question = input(\"add some word in questions : \")\n",
    "    difficulty = input(\"difficulty(e,m,h,f): \")\n",
    "    print(difficulty + '\\n')\n",
    "\n",
    "    return answer, question + '' + add_question, difficulty.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_extract(dir):\n",
    "    document = [] \n",
    "    with open(\"documents\"+dir, \"r\") as file:\n",
    "        while True:\n",
    "            line = file.readline()\n",
    "            if not line:\n",
    "                break\n",
    "            if line != '\\n' and line[0] != '=':\n",
    "                sentence = line.strip('\\n').split(\". \")\n",
    "                document.extend(sentence)\n",
    "    \n",
    "    return document\n",
    "\n",
    "def sentence_extract2(dir):\n",
    "    document = [] \n",
    "    with open(\"documents\"+dir, \"r\") as file:\n",
    "        while True:\n",
    "            line = file.readline()\n",
    "            if not line:\n",
    "                break\n",
    "            if line != '\\n' and line[0] != '=':\n",
    "                sentence = line.strip('\\n').split(\". \")\n",
    "                document.extend(sentence)\n",
    "\n",
    "    two_sentence = ''\n",
    "    new_document = []\n",
    "    for i in document:\n",
    "        if two_sentence == '':\n",
    "            two_sentence = i\n",
    "        else:\n",
    "            new_document.append(two_sentence + '. ' + i + '.')\n",
    "            two_sentence = ''\n",
    "    \n",
    "    return new_document\n",
    "\n",
    "def sentence_extract3(dir):\n",
    "    document = [] \n",
    "    with open(\"documents\"+dir, \"r\") as file:\n",
    "        while True:\n",
    "            line = file.readline()\n",
    "            if not line:\n",
    "                break\n",
    "            if line != '\\n' and line[0] != '=':\n",
    "                sentence = line.strip('\\n').split(\". \")\n",
    "                document.extend(sentence)\n",
    "\n",
    "    num_sentence = 0\n",
    "    sentence = ''\n",
    "    new_document = []\n",
    "    for i in document:\n",
    "        if num_sentence == 0:\n",
    "            sentence = i\n",
    "            num_sentence += 1\n",
    "        elif num_sentence < 2:\n",
    "            sentence = sentence + '. ' + i\n",
    "            num_sentence += 1\n",
    "        else:\n",
    "            new_document.append(sentence + '. ' + i)\n",
    "            sentence = ''\n",
    "            num_sentence = 0\n",
    "    \n",
    "    return new_document\n",
    "dir = \"/pokemon_characters/eevee_pokemon.txt\"\n",
    "document = sentence_extract3(dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context: Author Loredana Lipperini noted Eevee as being one of the \"most mysterious PokÃ©mon in the series\". Technology Tell writer Jenni Lada noted that Eevee is a character \"whom should feature\" in Super Smash Bros., citing its customization \"potential\". Liz Finnegan of The Escapist listed Eevee as their eleventh favorite PokÃ©mon, stating that Eevee is like the evolutionary equivalent of the turducken\n",
      "yes\n",
      "question: Is Eevee a good character for the Super Smash Bros.?\n",
      "e\n",
      "\n",
      "context: Gita Jackson of Kotaku claimed that Eevee is what PokÃ©mon is all about, stating that \"Eevee is adorable, but thatâ€™s not the only reason why I love them. Somewhere between a cat and a dog, Eevee seems more domesticated than longtime series mascot Pikachu, but still as mischievous as any other wild animal.\"In 2015, Eevee was the most traded PokÃ©mon in the games' \"Wonder Trade\" feature. A special PokÃ©mon Omega Ruby and Alpha Sapphire Online Competition known as the \"Eevee Friendly Match\" limits participants' PokÃ©mon to Eevee and its evolutions only\n",
      "yes\n",
      "question: Is Eevee a PokÃ©mon?\n",
      "e\n",
      "\n"
     ]
    }
   ],
   "source": [
    "answers = []\n",
    "questions = []\n",
    "documents = []\n",
    "difficulties = []\n",
    "for i, sentence in enumerate(document):\n",
    "    if i < 20:\n",
    "        continue\n",
    "    elif i == 22:\n",
    "        break\n",
    "    a, q, d = qa_generation(sentence)\n",
    "\n",
    "    if d =='F':\n",
    "        continue\n",
    "        \n",
    "    answers.append(a)\n",
    "    questions.append(q)\n",
    "    difficulties.append(d) # E, M, H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict = {'document_id': dir, 'question': questions, 'answer': answers, 'difficulty': difficulties}  \n",
    "df = pd.DataFrame(dict) \n",
    "df.to_csv('test.csv', index=False)  \n",
    "#df.to_csv('test.tsv', index=False, sep=\"\\t\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv ('result_4.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "M    40\n",
       "E    30\n",
       "H    30\n",
       "Name: difficulty, dtype: int64"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1['difficulty'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('NLP')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c6584a8ba916643306bb5090a7c6d906392e220f89e7f1e5ab1565c1ede3c150"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
