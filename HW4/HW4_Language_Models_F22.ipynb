{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "25d624c1",
      "metadata": {
        "id": "25d624c1"
      },
      "source": [
        "# 11411/611 – NLP (S22)\n",
        "## HW3 – Language Models"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ab1cb6a9",
      "metadata": {
        "id": "ab1cb6a9"
      },
      "source": [
        "**File version:** 1.0.3"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8710365b",
      "metadata": {
        "id": "8710365b"
      },
      "source": [
        "Whether it is transcribing spoken utterances as correct word sequences or generating coherent human-like text, language models are extremely useful.\n",
        "\n",
        "In this assignment, you will be building your own language model powered by n-grams."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d9cda894",
      "metadata": {
        "id": "d9cda894"
      },
      "source": [
        "### There are two major components in this HW:\n",
        "#### Part 1: Programming [60 marks]\n",
        "You are required to program an n-gram language model.\n",
        "\n",
        "#### Part 2: Analyses [40 marks]\n",
        "After writing the code, you are required to answer the empirical questions in the handout"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e485bc65",
      "metadata": {
        "id": "e485bc65"
      },
      "source": [
        "### Submission Guidelines\n",
        "\n",
        "**Deadline:** February 17th, 2022 at 11:59pm EST"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e9f727d4",
      "metadata": {
        "id": "e9f727d4"
      },
      "source": [
        "**Programming:** \n",
        "- This notebook contains helpful test cases and additional information about the programming part of the HW. However, you are only required to submit `lm.py` and `utils.py` on Gradescope.\n",
        "- We recommended that you first code in the notebook and then copy the corresponding methods/classes to `lm.py`.\n",
        "\n",
        "**Written:**\n",
        "- Analyses questions would require you to run your code.\n",
        "- You will submit the seperate written pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "wjvLk7XPMxnl",
      "metadata": {
        "id": "wjvLk7XPMxnl"
      },
      "source": [
        "### Downloading `utils.py` and data files (`data/`)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "gvwPMR2kNBvR",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gvwPMR2kNBvR",
        "outputId": "38cccfcd-b0fa-4ae4-db43-622575fd7568"
      },
      "outputs": [],
      "source": [
        "!wget http://syllable.lti.cs.cmu.edu/nlp/hw3.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bmEQByNzNI4d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bmEQByNzNI4d",
        "outputId": "2778b1f9-3036-4397-937a-66c8fe097452"
      },
      "outputs": [],
      "source": [
        "! unzip hw3.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1f84134d",
      "metadata": {
        "id": "1f84134d"
      },
      "source": [
        "## Part 1: Language Models [60 points]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4d0b3c6b",
      "metadata": {
        "id": "4d0b3c6b"
      },
      "source": [
        "### Step 0: Importing essential libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "30667bf4",
      "metadata": {
        "id": "30667bf4"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "from itertools import product\n",
        "import math"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3d5ca5b6",
      "metadata": {
        "id": "3d5ca5b6"
      },
      "source": [
        "### Step 1: Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b3a6eedf",
      "metadata": {
        "id": "b3a6eedf"
      },
      "source": [
        "We provide you with a few functions in `utils.py` to read and preprocess your input data. Do not edit this file!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "195db6a3",
      "metadata": {
        "id": "195db6a3"
      },
      "outputs": [],
      "source": [
        "from utils import *"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f4ea47b0",
      "metadata": {
        "id": "f4ea47b0"
      },
      "source": [
        "We have performed a round of preprocessing on the datasets.\n",
        "\n",
        "- Each file contains one sentence per line.\n",
        "- All punctuation marks have been removed.\n",
        "- Each line is a sequences of tokens separated by whitespace."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5fbe98cc",
      "metadata": {
        "id": "5fbe98cc"
      },
      "source": [
        "#### Special Symbols ( Already defined in `utils.py` )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ed9e54f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ed9e54f",
        "outputId": "9c7fe1e8-ff9c-4271-e5b6-b3c345d88f54"
      },
      "outputs": [],
      "source": [
        "print(\"Sentence START symbol: {}\".format(START))\n",
        "print(\"Sentence END symbol: {}\".format(EOS))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6b1f4a6f",
      "metadata": {
        "id": "6b1f4a6f"
      },
      "source": [
        "#### Reading and processing an example file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d60ce7c2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d60ce7c2",
        "outputId": "39fa4844-051b-4c6c-db28-c95cf12e628f"
      },
      "outputs": [],
      "source": [
        "# Preprocessing example for bigrams (n=2)\n",
        "sample = read_file(\"data/sample.txt\")\n",
        "print(sample)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ec373cc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ec373cc",
        "outputId": "6a24253f-6927-4763-8b8f-2a70faead32c"
      },
      "outputs": [],
      "source": [
        "# Checkout utils.py to know more about the methods\n",
        "sample = preprocess(sample, n=2)\n",
        "print(sample)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d264145e",
      "metadata": {
        "id": "d264145e"
      },
      "source": [
        "### Step 2: Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "1322297b",
      "metadata": {
        "id": "1322297b"
      },
      "outputs": [],
      "source": [
        "def flatten(lst):\n",
        "    \"\"\"\n",
        "    Flattens a nested list into a 1D list.\n",
        "    Args:\n",
        "        lst: Nested list (2D)\n",
        "    \n",
        "    Returns:\n",
        "        Flattened 1-D list\n",
        "    \"\"\"\n",
        "    \n",
        "    return [item for sublist in lst for item in sublist]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b51edd23",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b51edd23",
        "outputId": "a094ac95-45b2-4ce7-abf7-184cdece681e"
      },
      "outputs": [],
      "source": [
        "print(flatten([[\"a\", \"b\", \"c\"], [\"d\"]]))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9165b404",
      "metadata": {
        "id": "9165b404"
      },
      "source": [
        "### Step 3: Get started!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5a4f73b0",
      "metadata": {
        "id": "5a4f73b0"
      },
      "source": [
        "### TO DO: `get_ngrams()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "id": "138c35b6",
      "metadata": {
        "id": "138c35b6"
      },
      "outputs": [],
      "source": [
        "#######################################\n",
        "# TO-DO: get_ngrams()\n",
        "#######################################\n",
        "def get_ngrams(list_of_words, n):\n",
        "    \"\"\"\n",
        "    Returns a list of n-grams for a list of words.\n",
        "    Args\n",
        "    ----\n",
        "    list_of_words: List[str]\n",
        "        List of already preprocessed and flattened (1D) list of tokens e.g. [\"<s>\", \"hello\", \"</s>\", \"<s>\", \"bye\", \"</s>\"]\n",
        "    n: int\n",
        "        n-gram order e.g. 1, 2, 3\n",
        "    \n",
        "    Returns:\n",
        "        n_grams: List[Tuple]\n",
        "            Returns a list containing n-gram tuples\n",
        "    \"\"\"\n",
        "    return NotImplemented"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5fdab35a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        },
        "id": "5fdab35a",
        "outputId": "fb9eeae6-f228-45c1-976a-4c16b91cd301"
      },
      "outputs": [],
      "source": [
        "assert get_ngrams(flatten(sample), 3) == [('<s>', 'we', 'are'), ('we', 'are', 'never'), ('are', 'never', 'ever'), ('never', 'ever', 'ever'), ('ever', 'ever', 'ever'), ('ever', 'ever', 'ever'), ('ever', 'ever', 'getting'), ('ever', 'getting', 'back'), ('getting', 'back', 'together'), ('back', 'together', '</s>'), ('together', '</s>', '<s>'), ('</s>', '<s>', 'we'), ('<s>', 'we', 'are'), ('we', 'are', 'the'), ('are', 'the', 'ones'), ('the', 'ones', 'together'), ('ones', 'together', 'we'), ('together', 'we', 'are'), ('we', 'are', 'back'), ('are', 'back', '</s>')]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7bbb4a88",
      "metadata": {
        "id": "7bbb4a88"
      },
      "source": [
        "### **TO DO:** Class `LanguageModel()`"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f7f174c9",
      "metadata": {
        "id": "f7f174c9"
      },
      "source": [
        "Now, we will define our LanguageModel class.\n",
        "\n",
        "**Required parameters:**\n",
        "- self.model: `dict` of n-grams and their corresponding probabilities.\n",
        "- self.vocab: `dict` of unigram vocabulary with counts.\n",
        "- self.n: `int` value for n-gram order (e.g. 1,2,3).\n",
        "- self.train_data: `List[List]` containing preprocessed train sentences.\n",
        "- self.alpha: `float` indicates the lazy laplace smoothing parameter (Can theoritically take any non-negative real value, stick to values between 0 and 1 for now).\n",
        "\n",
        "In `lm.py`, we will be taking most of these argumemts from command line using this command:\n",
        "\n",
        "`python3 lm.py --train data/sample.txt --test data/sample.txt --n 3 --alpha 0`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "id": "24f2ce5c",
      "metadata": {
        "id": "24f2ce5c"
      },
      "outputs": [],
      "source": [
        "#######################################\n",
        "# TO-DO: LanguageModel()\n",
        "#######################################\n",
        "class LanguageModel():\n",
        "    def __init__(self, n, train_data, alpha=1):\n",
        "        \"\"\"\n",
        "        Language model class.\n",
        "        \n",
        "        Args\n",
        "        ____\n",
        "        n: int\n",
        "            n-gram order\n",
        "        train_data: List[List]\n",
        "            already preprocessed list of sentences. e.g. [[\"<s>\", \"hello\", \"my\", \"</s>\"], [\"<s>\", \"hi\", \"there\", \"</s>\"]]\n",
        "        alpha: float\n",
        "            Smoothing parameter\n",
        "        \n",
        "        Other required parameters:\n",
        "            self.vocab: vocabulary dict with counts\n",
        "            self.model: n-gram language model, i.e., n-gram dict with probabilties\n",
        "            self.n_grams_counts: Frequency count for each of the n-grams present in the training data\n",
        "            self.prefix_counts: Frequency count of all the corresponding n-1 grams present in the training data\n",
        "        \"\"\"\n",
        "        self.n = n\n",
        "        self.train_data = train_data\n",
        "        self.tokens = flatten(self.train_data)\n",
        "        self.n_grams_counts = None\n",
        "        self.prefix_counts = None\n",
        "        self.vocab  = Counter(self.tokens)\n",
        "        self.alpha = alpha\n",
        "        self.model = self.build()\n",
        "\n",
        "    #TODO:\n",
        "    def get_smooth_probabilites(self,n_gram):\n",
        "        \"\"\"\n",
        "        Returns the smoothed probability of a single ngram, using Laplace Smoothing. Remember to handle the special case of n=1\n",
        "        Use the class variables we defined in the build function. It is suggested to implement the build function before this one.\n",
        "        \"\"\"\n",
        "        \n",
        "        return NotImplementedError\n",
        "    \n",
        "    #TODO:\n",
        "    def build(self):\n",
        "        \"\"\"\n",
        "        Returns a n-gram (could be a unigram) dict with n-gram tuples as keys and probabilities as values. \n",
        "        It could be a unigram model as well\n",
        "        \"\"\"\n",
        "        \n",
        "        # TODO: Get the n-grams from the training data using the previously defined methods\n",
        "        \n",
        "        # TODO: Define the class variables n_grams_counts and prefix_counts \n",
        "        self.n_grams_counts = None\n",
        "        self.prefix_counts = None\n",
        "\n",
        "        # TODO Get the Probabilities using the get_smooth_probabilities\n",
        "        prob = None\n",
        "\n",
        "        return NotImplemented"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "id": "909b9c4a",
      "metadata": {
        "id": "909b9c4a"
      },
      "outputs": [],
      "source": [
        "# Quick test\n",
        "test_lm = LanguageModel(n=2, train_data=sample, alpha=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "id": "600UKf6JM_4n",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "600UKf6JM_4n",
        "outputId": "445f1034-59a1-4591-af74-b8e603f71059"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "NotImplemented"
            ]
          },
          "execution_count": 82,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_lm.model\n",
        "# test_lm.vocab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "501ac225",
      "metadata": {
        "id": "501ac225"
      },
      "outputs": [],
      "source": [
        "assert test_lm.vocab == Counter({'<s>': 2,'we': 3,'are': 3,'never': 1,'ever': 4,'getting': 1,'back': 2,'together': 2,'</s>': 2,'the': 1,'ones': 1})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "157b0749",
      "metadata": {
        "id": "157b0749"
      },
      "outputs": [],
      "source": [
        "assert test_lm.model =={('<s>', 'we'): 1.0, ('we', 'are'): 1.0, ('are', 'never'): 0.3333333333333333, ('never', 'ever'): 1.0,('ever', 'ever'): 0.75, ('ever', 'getting'): 0.25,('getting', 'back'): 1.0,('back', 'together'): 0.5,('together', '</s>'): 0.5,('</s>', '<s>'): 0.5, ('are', 'the'): 0.3333333333333333,('the', 'ones'): 1.0,('ones', 'together'): 1.0,('together', 'we'): 0.5,('are', 'back'): 0.3333333333333333, ('back', '</s>'): 0.5}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "edd7f21f",
      "metadata": {
        "id": "edd7f21f"
      },
      "source": [
        "### **TO DO:**  `perplexity()`"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fe510005",
      "metadata": {
        "id": "fe510005"
      },
      "source": [
        "Steps:\n",
        "1. First, create n-grams after flattening the data.\n",
        "2. If a matching n-gram is not found, perform Lazy Laplace Smoothing. Remember to memoize the new n-gram probabilities to speed up calculation\n",
        "3. Refer to the perplexity equation in the language model chapter of the textbook required for this course. [[Link to the chapter]](https://web.stanford.edu/~jurafsky/slp3/3.pdf)\n",
        "\n",
        "Tips:\n",
        "- Remember that product changes summation under `log`. Take log of probabilities (`math.log()`), sum them up (`sum()`) and then exponentiate it (`math.exp()`) to get back to the original scale.\n",
        "- Make sure to `flatten()` your data before creating the n_grams using `get_ngrams().`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "id": "041d12b1",
      "metadata": {
        "id": "041d12b1"
      },
      "outputs": [],
      "source": [
        "#######################################\n",
        "# TO-DO: perplexity()\n",
        "#######################################\n",
        "def perplexity(lm, test_data):\n",
        "    \"\"\"\n",
        "    Returns perplexity calculated on the test data.\n",
        "    Args\n",
        "    ----------\n",
        "    test_data: List[List] \n",
        "        Already preprocessed nested list of sentences\n",
        "        \n",
        "    lm: LanguageModel class object\n",
        "        To be used for retrieving lm.model, lm.n and lm.vocab\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    float\n",
        "        Calculated perplexity value\n",
        "    \"\"\"\n",
        "    # TODO Flatten and get the n-grams\n",
        "\n",
        "    perplexity = None\n",
        "    # TODO Calculate the Perplexity over all the test n-grams\n",
        "    return NotImplemented"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2927e9aa",
      "metadata": {
        "id": "2927e9aa"
      },
      "outputs": [],
      "source": [
        "# Quick test\n",
        "test_lm = LanguageModel(n=2, train_data=sample, alpha=0)\n",
        "test_ppl = perplexity(test_lm, sample)\n",
        "assert test_ppl < 1.7"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0212dc6c",
      "metadata": {
        "id": "0212dc6c"
      },
      "source": [
        "### Step 4: Bringing everything together!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "35874f48",
      "metadata": {
        "id": "35874f48"
      },
      "source": [
        "**Note:** Most of these will already be defined for you in `main()` method in `lm.py`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "fe0fcc1c",
      "metadata": {
        "id": "fe0fcc1c"
      },
      "outputs": [],
      "source": [
        "# Arguments\n",
        "\n",
        "train_path = \"data/bbc/tech.txt\"\n",
        "test_path = \"data/bbc/tech.txt\"\n",
        "n = 3\n",
        "min_freq = 1\n",
        "smoothing = 0.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "3c59236d",
      "metadata": {
        "id": "3c59236d"
      },
      "outputs": [],
      "source": [
        "train = read_file(train_path)\n",
        "test = read_file(test_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b1c4ad51",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b1c4ad51",
        "outputId": "a627b46a-174a-49a9-fd4b-2a6fcc9a1f30"
      },
      "outputs": [],
      "source": [
        "print(\"No of sentences in train file: {}\".format(len(train)))\n",
        "print(\"No of sentences in test file: {}\".format(len(test)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "271bd85a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "271bd85a",
        "outputId": "67b3aef3-a66d-4283-de44-27ecd8af250b"
      },
      "outputs": [],
      "source": [
        "print(\"Raw train example: \\n{}\".format(train[2]))\n",
        "print(\"Raw test example: \\n{}\".format(test[2]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "d9edfdb7",
      "metadata": {
        "id": "d9edfdb7"
      },
      "outputs": [],
      "source": [
        "# Basic preprocessing\n",
        "train = preprocess(train, n)\n",
        "test = preprocess(test, n)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f0a63a6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9f0a63a6",
        "outputId": "0a90fb91-576a-4ce3-c23c-d096fc950bab"
      },
      "outputs": [],
      "source": [
        "print(\"Preprocessed train example: \\n{}\\n\".format(train[2]))\n",
        "print(\"Preprocessed test example: \\n{}\".format(test[2]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a85f6b6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4a85f6b6",
        "outputId": "007ca862-7690-49cd-cf12-096b04eb91bf"
      },
      "outputs": [],
      "source": [
        "print(\"Loading {}-gram model.\".format(n))\n",
        "lm = LanguageModel(n, train, smoothing)\n",
        "print(\"Vocabulary size (unique unigrams): {}\".format(len(lm.vocab)))\n",
        "print(\"Total number of unique n-grams: {}\".format(len(lm.model)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "22f11d60",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "22f11d60",
        "outputId": "1de0bef0-9c03-4491-f862-3ed6e1acce05"
      },
      "outputs": [],
      "source": [
        "# Calculate perplexity\n",
        "\n",
        "ppl = perplexity(lm, test)\n",
        "print(\"Model perplexity: {:.3f}\".format(ppl))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bfab26d5",
      "metadata": {
        "id": "bfab26d5"
      },
      "source": [
        "### Step 5: Experimenting with generations!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "057decc9",
      "metadata": {
        "id": "057decc9"
      },
      "source": [
        "**Note**: These methods are already written for you. Use them to solve Written questions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "2289b988",
      "metadata": {
        "id": "2289b988"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "random.seed(11411)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "ee2382a8",
      "metadata": {
        "id": "ee2382a8"
      },
      "outputs": [],
      "source": [
        "def best_candidate(lm, prev, i, without=[], mode=\"random\"):\n",
        "    \"\"\"\n",
        "    Returns the most probable word candidate after a given sentence.\n",
        "    \"\"\"\n",
        "    blacklist  = [\"<UNK>\"] + without\n",
        "    candidates = ((ngram[-1],prob) for ngram,prob in lm.model.items() if ngram[:-1]==prev)\n",
        "    candidates = filter(lambda candidate: candidate[0] not in blacklist, candidates)\n",
        "    candidates = sorted(candidates, key=lambda candidate: candidate[1], reverse=True)\n",
        "    if len(candidates) == 0:\n",
        "        return (\"</s>\", 1)\n",
        "    else:\n",
        "        if(mode==\"random\"):\n",
        "            return candidates[random.randrange(len(candidates))]\n",
        "        else:\n",
        "            return candidates[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "f1c51278",
      "metadata": {
        "id": "f1c51278"
      },
      "outputs": [],
      "source": [
        "def top_k_best_candidates(lm, prev, k, without=[]):\n",
        "    \"\"\"\n",
        "    Returns the K most-probable word candidate after a given n-1 gram.\n",
        "    Args\n",
        "    ----\n",
        "    lm: LanguageModel class object\n",
        "    \n",
        "    prev: n-1 gram\n",
        "        List of tokens n\n",
        "    \"\"\"\n",
        "    blacklist  = [\"<UNK>\"] + without\n",
        "    candidates = ((ngram[-1],prob) for ngram,prob in lm.model.items() if ngram[:-1]==prev)\n",
        "    candidates = filter(lambda candidate: candidate[0] not in blacklist, candidates)\n",
        "    candidates = sorted(candidates, key=lambda candidate: candidate[1], reverse=True)\n",
        "    if len(candidates) == 0:\n",
        "        return (\"</s>\", 1)\n",
        "    else:\n",
        "        return candidates[:k]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "9fe5444d",
      "metadata": {
        "id": "9fe5444d"
      },
      "outputs": [],
      "source": [
        "def generate_sentences_from_phrase(lm, num, sent, prob, mode):\n",
        "    \"\"\"\n",
        "    Generate sentences using the trained language model.\n",
        "    \"\"\"\n",
        "    min_len=12\n",
        "    max_len=24\n",
        "    \n",
        "    for i in range(num):\n",
        "        while sent[-1] != \"</s>\":\n",
        "            prev = () if lm.n == 1 else tuple(sent[-(lm.n-1):])\n",
        "            blacklist = sent + ([\"</s>\"] if len(sent) < min_len else [])\n",
        "\n",
        "            next_token, next_prob = best_candidate(lm, prev, i, without=blacklist, mode=mode)\n",
        "            sent.append(next_token)\n",
        "            prob *= next_prob\n",
        "            \n",
        "            if len(sent) >= max_len:\n",
        "                sent.append(\"</s>\")\n",
        "\n",
        "        yield ' '.join(sent), -1/math.log(prob)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "14ba8069",
      "metadata": {
        "id": "14ba8069"
      },
      "source": [
        "## Part 2: Written [40 points]. We have given some code for some of the written parts to make it easier for you."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "53431996",
      "metadata": {
        "id": "53431996"
      },
      "source": [
        "### **Written 3.3** – Song Attribution [8 points]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4751ea5b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4751ea5b",
        "outputId": "09ecb62d-bafa-477f-c191-4f5d3512b662"
      },
      "outputs": [],
      "source": [
        "# Example code for Taylor Swift LM\n",
        "n = 3\n",
        "smoothing = 0.1\n",
        "min_freq = 1\n",
        "\n",
        "train = read_file(\"data/lyrics/taylor_swift.txt\")\n",
        "test = read_file(\"data/lyrics/test_lyrics.txt\")\n",
        "\n",
        "train = preprocess(train, n)\n",
        "test = preprocess(test, n)\n",
        "lm = LanguageModel(n, train, smoothing)\n",
        "\n",
        "ppl = perplexity(lm, test)\n",
        "print(ppl)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "749a49a0",
      "metadata": {
        "id": "749a49a0"
      },
      "source": [
        "### **Written 3.4.1** –  Intro to Decoding [8 points]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "id": "c8ed9b85",
      "metadata": {
        "id": "c8ed9b85"
      },
      "outputs": [],
      "source": [
        "n = 3\n",
        "smoothing = 0.1\n",
        "min_freq = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "id": "96b3d000",
      "metadata": {
        "id": "96b3d000"
      },
      "outputs": [],
      "source": [
        "train = read_file(\"data/bbc/entertainment.txt\")\n",
        "train = preprocess(train, n)\n",
        "\n",
        "lm = LanguageModel(n, train, smoothing)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "id": "17794ab7",
      "metadata": {
        "id": "17794ab7"
      },
      "outputs": [],
      "source": [
        "s1 = (\"number\", \"three\")\n",
        "\n",
        "s2 = (\"starred\", \"in\")\n",
        "\n",
        "s3 = (\"actor\", \"in\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "id": "69ef66a2",
      "metadata": {
        "id": "69ef66a2"
      },
      "outputs": [],
      "source": [
        "# TODO Get the top 5 candidates for next best word\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8ee4e956",
      "metadata": {
        "id": "8ee4e956"
      },
      "source": [
        "### **Written 3.4.2** – Text Generation [8 points]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "205129a7",
      "metadata": {
        "id": "205129a7"
      },
      "source": [
        "For this subtask, use the LM trained in Written 3.4.1\n",
        "\n",
        "Selecting words sequentially from a probability distribution is called _decoding_.\n",
        "\n",
        "Two popular decoding approaches are,\n",
        "1. **Max-probability decoding** - We consistently choose the candidate with maximum probability.\n",
        "2. **Random Sampling** - We sample a candidate randomly.\n",
        "2. **top-K Sampling** - We sample a candidate randomly from the top-K most probable choices.\n",
        "\n",
        "In this part, we will try the first two approaches to generate sentences.\n",
        "\n",
        "Q1. Use `generate_sentences()` method to generate sentences after the provided phrases from `s1` to `s3`. Use modes `random` and `max`. Report one of your favourite generations (for any strategy or phrase). Which mode you think is better and why?\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dcd3a291",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dcd3a291",
        "outputId": "803606c0-975f-441f-b881-b0cbbdeb80dc"
      },
      "outputs": [],
      "source": [
        "# Random\n",
        "for _ in range(5):\n",
        "    print(list(generate_sentences_from_phrase(lm, 1, [\"<s>\", \"<s>\", \"number\", \"three\"], 0.2, mode=\"random\")))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d712619f",
      "metadata": {
        "id": "d712619f"
      },
      "source": [
        "**Aside (for fun!)**: Train your LM on Taylor Swift lyrics and generate the next hit!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "17b9047d",
      "metadata": {
        "id": "17b9047d"
      },
      "source": [
        "### **Written 3.5** – Battle of the LMs: GPT-2 vs Trigram [8 points]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "41d38359",
      "metadata": {
        "id": "41d38359"
      },
      "source": [
        "For this subtask, you will be generating text and comparing GPT-2 with your n-gram language model. \n",
        "\n",
        "Generative pretrained transformer (GPT) is a neural language model series created by OpenAI. The n-gram language model you trained has on average around 10K-20K parameters (`len(lm.model)`.) Compare that to the 175 billion parameters of the latest version of GPT-3!\n",
        "\n",
        "Let's see how GPT-2 compares to the LM you trained in subtask 4 on `data/bbc/tech-small.txt` dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e143e87c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e143e87c",
        "outputId": "c429800e-7cb2-4ce2-e25f-e9d349422a66"
      },
      "outputs": [],
      "source": [
        "test = read_file(\"data/bbc/tech-small.txt\")\n",
        "\n",
        "\n",
        "#TODO: Calculate your n-gram model's perplexity\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db542ceb",
      "metadata": {
        "id": "db542ceb"
      },
      "source": [
        "### Computing GPT-2's perplexity on test set\n",
        "\n",
        "You need to enable a GPU runtime from `Runtime` menu option. Go to `Runtime` → `Change Runtime Type` → `Hardware Accelerator (GPU)`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "id": "d6501dcd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d6501dcd",
        "outputId": "d01f3383-831f-4b1f-ff59-c8a70e4b8e4b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.22.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.12.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.9.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.12.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.9.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.6.15)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.12.1+cu113)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (4.1.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (4.64.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "!pip install torch\n",
        "!pip install tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "id": "Ldf6ovg4B5Qc",
      "metadata": {
        "id": "Ldf6ovg4B5Qc"
      },
      "outputs": [],
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2TokenizerFast\n",
        "import torch\n",
        "\n",
        "model_id = \"distilgpt2\"\n",
        "model = GPT2LMHeadModel.from_pretrained(model_id)\n",
        "tokenizer = GPT2TokenizerFast.from_pretrained(model_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "67zwTHSI0hCC",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "67zwTHSI0hCC",
        "outputId": "0130ddd6-c2e3-4b57-c6ae-11b39e5a8703"
      },
      "outputs": [],
      "source": [
        "test = read_file(\"data/bbc/tech-small.txt\")\n",
        "encodings = tokenizer(\"\\n\\n\".join(test), return_tensors=\"pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "wmQKbMXjDFNj",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wmQKbMXjDFNj",
        "outputId": "d28c273f-7b74-4d38-e9c3-16e982aebf2f"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "max_length = model.config.n_positions\n",
        "stride = 100\n",
        "\n",
        "nlls = []\n",
        "for i in tqdm(range(0, encodings.input_ids.size(1), stride)):\n",
        "    begin_loc = max(i + stride - max_length, 0)\n",
        "    end_loc = min(i + stride, encodings.input_ids.size(1))\n",
        "    trg_len = end_loc - i  # may be different from stride on last loop\n",
        "    input_ids = encodings.input_ids[:, begin_loc:end_loc]\n",
        "    target_ids = input_ids.clone()\n",
        "    target_ids[:, :-trg_len] = -100\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids, labels=target_ids)\n",
        "        neg_log_likelihood = outputs[0] * trg_len\n",
        "\n",
        "    nlls.append(neg_log_likelihood)\n",
        "\n",
        "ppl = torch.exp(torch.stack(nlls).sum() / end_loc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "giXGq0Z0DWdr",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "giXGq0Z0DWdr",
        "outputId": "cfc34c8b-5850-4c51-a926-148045f4e51f"
      },
      "outputs": [],
      "source": [
        "print(\"Perplexity using GPT2:\", ppl.item())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "id": "a5ZGKl-6s3AF",
      "metadata": {
        "id": "a5ZGKl-6s3AF"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
